{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para importar la base de datos leucemia\n",
    "from sklearn.datasets.mldata import fetch_mldata\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Para Procesamiento de datos\n",
    "## Regresion logistica\n",
    "from sklearn.linear_model import LogisticRegression   #Importa las fxn de validacion cruzadas\n",
    "from sklearn.cross_validation import train_test_split #Importa las funciones de validación cruzada\n",
    "from sklearn.preprocessing import StandardScaler      #Importar las funciones de preparacion \n",
    "## Naive bayes\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB #importar libreria del clasificador \n",
    "## Clustering\n",
    "# K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster\n",
    "## GMM\n",
    "from matplotlib import patches # es para hacer elipses\n",
    "from sklearn import datasets\n",
    "from sklearn.mixture import GMM #mixture contiene los modelos de mezclas\n",
    "from sklearn.cross_validation import StratifiedKFold #validación por K-folds\n",
    "## Arboles \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Supervisados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__________________________________ Preprocesamiento de datos ___________________________________#\n",
    "def Preprocesamiento_datos(X,y):\n",
    "    # Remocion de la media\n",
    "    X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.3, random_state = 3)\n",
    "\n",
    "    #Remoción para x_train\n",
    "    def Remocion(X):\n",
    "        med = X.mean(axis=0)\n",
    "        des = X.std(axis=0)\n",
    "        X = X - med\n",
    "        X = X/des\n",
    "        return X,med, des\n",
    "\n",
    "    #Remocion para x_test\n",
    "    def Remocion_test(X,med,des):\n",
    "        X = X - med\n",
    "        X = X/des\n",
    "        return X\n",
    "\n",
    "    #Remocion para x_train\n",
    "    x_norm_train,t_train_mean,t_train_desv = Remocion(X_train)\n",
    "    #Remocion para x_test\n",
    "    x_norm_test = Remocion_test(X_test,t_train_mean,t_train_desv)\n",
    "\n",
    "    clasificador = LogisticRegression(C=1000.0,random_state=0) # C es el parametro\n",
    "    clasificador.fit(x_norm_train, y_train) # entrenamiento del clasificador\n",
    "\n",
    "    # para validar el clasificador\n",
    "    y_pred = clasificador.predict(x_norm_test)\n",
    "\n",
    "    # Para visualizar los resultados y analizarlos: \n",
    "    print('Las muestras mal clasificadas fueron de %d/%d '%((y_test == y_pred).sum(),len(y_pred)))\n",
    "    val = ((y_test == y_pred).sum()/len(y_pred))*100\n",
    "    print('El porcentaje de muestras bien clasificadas es de : ',val,'%')\n",
    "    validar_clasificador(y_train,X_train,clasificador,y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________________________ Naive Bayes ____________________________________#\n",
    "def Naive_Bayes(X,y):\n",
    "    # Dividimos la base de datos\n",
    "    X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.3, random_state = 3)\n",
    "\n",
    "    #generar el clasificador\n",
    "    clasificador = GaussianNB() #instanciamos y luego entrenamos\n",
    "    clasificador.fit(X_train, y_train) #entrenamos el clasificador\n",
    "\n",
    "    #hacer la predicion\n",
    "    y_predict = clasificador.predict(X_test)\n",
    "\n",
    "    # Entregar los resultados: \n",
    "    print('Las muestras mal clasificadas fueron de %d/%d '%((y_test == y_predict).sum(),len(y_predict)))\n",
    "    val = ((y_test == y_predict).sum()/len(y_predict))*100\n",
    "    print('El porcentaje de muestras bien clasificadas es de : ',val,'%')\n",
    "    validar_clasificador(y_train,X_train,clasificador,y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje No Supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__________________________ K-Means ______________________________________#\n",
    "\n",
    "def k_means_Cluster(X,y): \n",
    "    # definimos el número de clusters o grupos\n",
    "    num_cluster = 2\n",
    "    #generamos el modelo \n",
    "\n",
    "    #init establece el método de localizacion inicial de los centroides\n",
    "    modelo_kmeans= KMeans(init = 'k-means++',n_clusters = num_cluster, n_init=10)\n",
    "\n",
    "    #entrenar el modelo \n",
    "    modelo_kmeans.fit(X)\n",
    "\n",
    "    #centros\n",
    "    centroides = modelo_kmeans.cluster_centers_\n",
    "\n",
    "    labels = modelo_kmeans.labels_ \n",
    "    print('Etiquetas con Clustering : ')\n",
    "    print('______________________________________________________________________________')\n",
    "    print(labels)\n",
    "    print('Etiquetas originales: ')\n",
    "    print('______________________________________________________________________________')\n",
    "    print(y)\n",
    "    clase_0 = 0\n",
    "    clase_1 = 0\n",
    "\n",
    "    labels_t = np.where(labels==0,-1,labels)\n",
    "    print('______________________________________________________________________________')\n",
    "    print(labels_t)\n",
    "    cont = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels_t[i] == labels[i]:\n",
    "            cont = cont + 1\n",
    "    val = (cont/len(labels))*100\n",
    "\n",
    "    print('El porcentaje de muestras bien clasificadas es de : ',val,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de mezclas Gaussianas GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM(X,y):\n",
    "    # Utilizamos una variable externa para manipular las etiquetas y no alterar las originales\n",
    "    y_respaldo = y\n",
    "    y_respaldo = np.where(y_respaldo==-1,0,y_respaldo)\n",
    "\n",
    "    #print(y_respaldo)\n",
    "\n",
    "    #realizamos las particiones de los datos para entrenamiento y validacion 80/20\n",
    "    indices = StratifiedKFold(y_respaldo, n_folds = 5)\n",
    "\n",
    "    #objetos iterables para sacar los indices\n",
    "    train_index,test_index = next(iter(indices))\n",
    "\n",
    "    #extraer los datos de la base de datos\n",
    "    X_train = X[train_index]\n",
    "    y_train = y_respaldo[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y_respaldo[test_index]\n",
    "\n",
    "\n",
    "    #calculamos el número de clases\n",
    "    numero_clases = len(np.unique(y_train))\n",
    "    \n",
    "    # Construir el clasificador GMM\n",
    "    clasificador = GMM(n_components = numero_clases,covariance_type = 'full', init_params = 'w', n_iter = 20)\n",
    "\n",
    "    clasificador.means_ = np.array([X_train[y_train==i].mean(axis=0)for i in range(numero_clases)])\n",
    "\n",
    "    #Entrenamos con fit\n",
    "    clasificador.fit(X_train)\n",
    "    # Predecimos las etiquitas \n",
    "    y_train_predict = clasificador.predict(X_train)\n",
    "    # Calculamos el porcentaje de aciertos de las muestras bien entrenadas\n",
    "    acc_train = np.mean(y_train_predict.ravel() == y_train.ravel())*100\n",
    "    print('El acierto de entrenamiento es del ',acc_train,'%')\n",
    "\n",
    "    # Predecimos las etiquetas con los datos entrenados\n",
    "    y_test_predict = clasificador.predict(X_test)\n",
    "    # Calculamos el porcentaje de aciertos de las muestras de la prueba\n",
    "    acc_test = np.mean(y_test_predict.ravel() == y_test.ravel())*100\n",
    "    print('El acierto de pruebas es del ',acc_test,'%')\n",
    "\n",
    "    # Reporte de validación\n",
    "    validar_clasificador(y_train,X_train,clasificador,y_test,y_test_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arboles de decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Arbol_desicion(X,y):\n",
    "    # Separamos los datos de acuerdo a las etiquetas(2 clases), esto varia dependiendo del numero de clases\n",
    "    clase1 = np.array(X[y==1])\n",
    "    clase_1 = np.array(X[y==-1])\n",
    "\n",
    "    # Dividimos los datos en entrenamiento y validación\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y, test_size = 0.25, random_state = 5)\n",
    "\n",
    "    # construimos el clasificador\n",
    "    # variable tipo diccionario con los parametros del arbol\n",
    "    # random state: semilla con la que arranca las cosas aleatorias\n",
    "    # max_depth: Numero de niveles del arbolito.\n",
    "    parametros = {'random_state' : 0, 'max_depth':4}\n",
    "    #construir arbol de clasificación\n",
    "    clasificador = DecisionTreeClassifier(**parametros)\n",
    "\n",
    "    # lo entrenamos para que con el conjunto de entrenamiento se arme el arbolito\n",
    "    clasificador.fit(X_train, y_train)\n",
    "    # Predecimos las etiquetas\n",
    "    y_test_pred = clasificador.predict(X_test)\n",
    "    # Mostramos los resultados\n",
    "    val = ((y_test == y_test_pred).sum()/len(y_test_pred))*100\n",
    "    print('El porcentaje de muestras bien clasificadas es de : ',val,'%')\n",
    "    validar_clasificador(y_train,X_train,clasificador,y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosques aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bosques_aleatorios(X,y):    \n",
    "    # Separamos los datos de acuerdo a las etiquetas(2 clases), esto varia dependiendo del numero de clases\n",
    "    clase1 = np.array(X[y==1])\n",
    "    clase_1 = np.array(X[y==-1])\n",
    "\n",
    "    # Dividimos los datos en entrenamiento y validación\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y, test_size = 0.25, random_state = 5)\n",
    "\n",
    "    # hay dos tipos de bosques aleatorios, 'ba' y 'baa'.\n",
    "    # A continuación se define o se indica el tipo de arbol que queremos generar\n",
    "    tipo_clasificador = 'ba'\n",
    "    # Se crea un diccionario con los parametros o caracteristicas que debe de tener el árbol\n",
    "    parametros = { 'n_estimators':100, 'max_depth': 4, 'random_state':0}\n",
    "\n",
    "    # Se genere el bosque dependiendo de el parametro o tipo_clasificador que se necesita\n",
    "    if tipo_clasificador == 'ba':\n",
    "        # Bosque aleatorio\n",
    "        clasificador = RandomForestClassifier(**parametros)\n",
    "    else:\n",
    "        # Bosque extra aleatorio\n",
    "        clasificador = ExtraTreesClassifier(**parametros)\n",
    "\n",
    "    # Entrenamos el clasificador\n",
    "    clasificador.fit(X_train, y_train)\n",
    "\n",
    "    #validamos el clasificador\n",
    "    y_test_pred = clasificador.predict(X_test)\n",
    "\n",
    "    ## REPORTE DE CLASIFICACIÓN\n",
    "    val = ((y_test == y_test_pred).sum()/len(y_test_pred))*100\n",
    "    print('El porcentaje de muestras bien clasificadas es de : ',val,'%')\n",
    "    validar_clasificador(y_train,X_train,clasificador,y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporte de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funcion para el reporte de los diferentes metodos.\n",
    "def validar_clasificador(y_train,X_train,clasificador,y_test,y_predict):\n",
    "    # Evaluamos el desempeño del clasificador imprimiendo el reporte de clasificación.\n",
    "    nombres_clases = ['Primera Clase','Segunda Clase']\n",
    "    print('\\n' + '#'*70)\n",
    "    print('\\n Desempeño del clasificador sobre el conjunto de entrenamiento \\n')\n",
    "    print(classification_report(y_train, clasificador.predict(X_train), target_names = nombres_clases))\n",
    "    print('#'*70 + '\\n')\n",
    "    print('\\n'+ '#'*70 )\n",
    "    print('Desempeño del clasificador sobre el conjunto de la validación \\n')\n",
    "    print(classification_report(y_test,y_predict, target_names = nombres_clases))\n",
    "    print('\\n'+ '#'*70 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORTE Y CONCLUSIONES:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las base de datos\n",
    "test_data_home = tempfile.mkdtemp()\n",
    "leuk = fetch_mldata('leukemia', transpose_data=True, data_home=test_data_home)\n",
    "\n",
    "X = leuk['data']\n",
    "\n",
    "y = leuk['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "________________PROCESAMIENTO DE DATOS_____________\n",
      "Las muestras mal clasificadas fueron de 21/22 \n",
      "El porcentaje de muestras bien clasificadas es de :  95.4545454545 %\n",
      "\n",
      "######################################################################\n",
      "\n",
      " Desempeño del clasificador sobre el conjunto de entrenamiento \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Primera Clase       1.00      1.00      1.00        16\n",
      "Segunda Clase       1.00      1.00      1.00        34\n",
      "\n",
      "  avg / total       1.00      1.00      1.00        50\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Desempeño del clasificador sobre el conjunto de la validación \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Primera Clase       0.90      1.00      0.95         9\n",
      "Segunda Clase       1.00      0.92      0.96        13\n",
      "\n",
      "  avg / total       0.96      0.95      0.95        22\n",
      "\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "________________NAIVE BAYES________________________\n",
      "Las muestras mal clasificadas fueron de 22/22 \n",
      "El porcentaje de muestras bien clasificadas es de :  100.0 %\n",
      "\n",
      "######################################################################\n",
      "\n",
      " Desempeño del clasificador sobre el conjunto de entrenamiento \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Primera Clase       1.00      1.00      1.00        16\n",
      "Segunda Clase       1.00      1.00      1.00        34\n",
      "\n",
      "  avg / total       1.00      1.00      1.00        50\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Desempeño del clasificador sobre el conjunto de la validación \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Primera Clase       1.00      1.00      1.00         9\n",
      "Segunda Clase       1.00      1.00      1.00        13\n",
      "\n",
      "  avg / total       1.00      1.00      1.00        22\n",
      "\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "_________________Clustering________________________\n",
      "Etiquetas con Clustering : \n",
      "______________________________________________________________________________\n",
      "[1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1]\n",
      "Etiquetas originales: \n",
      "______________________________________________________________________________\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      " -1 -1 -1 -1  1  1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1]\n",
      "______________________________________________________________________________\n",
      "[ 1  1  1  1  1 -1 -1 -1  1 -1  1 -1  1  1  1  1  1 -1 -1  1 -1 -1 -1  1 -1\n",
      "  1 -1  1 -1  1 -1  1  1  1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1\n",
      " -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1  1  1 -1 -1  1]\n",
      "El porcentaje de muestras bien clasificadas es de :  41.66666666666667 %\n",
      "\n",
      "\n",
      "_________________Arboles de decisión_______________\n",
      "El porcentaje de muestras bien clasificadas es de :  94.4444444444 %\n",
      "\n",
      "######################################################################\n",
      "\n",
      " Desempeño del clasificador sobre el conjunto de entrenamiento \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Primera Clase       1.00      1.00      1.00        19\n",
      "Segunda Clase       1.00      1.00      1.00        35\n",
      "\n",
      "  avg / total       1.00      1.00      1.00        54\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Desempeño del clasificador sobre el conjunto de la validación \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Primera Clase       1.00      0.83      0.91         6\n",
      "Segunda Clase       0.92      1.00      0.96        12\n",
      "\n",
      "  avg / total       0.95      0.94      0.94        18\n",
      "\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "_________________Bosques aleatorios________________\n",
      "El porcentaje de muestras bien clasificadas es de :  100.0 %\n",
      "\n",
      "######################################################################\n",
      "\n",
      " Desempeño del clasificador sobre el conjunto de entrenamiento \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Primera Clase       1.00      1.00      1.00        19\n",
      "Segunda Clase       1.00      1.00      1.00        35\n",
      "\n",
      "  avg / total       1.00      1.00      1.00        54\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Desempeño del clasificador sobre el conjunto de la validación \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Primera Clase       1.00      1.00      1.00         6\n",
      "Segunda Clase       1.00      1.00      1.00        12\n",
      "\n",
      "  avg / total       1.00      1.00      1.00        18\n",
      "\n",
      "\n",
      "######################################################################\n",
      "\n",
      "\n",
      "_________________Mezclas Gausianas_________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GMM() got an unexpected keyword argument 'n_components'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-870f4fae8e89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mbosques_aleatorios\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n_________________Mezclas Gausianas_________________'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mGMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-682156daee2d>\u001b[0m in \u001b[0;36mGMM\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Construir el clasificador GMM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mclasificador\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumero_clases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcovariance_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'full'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mclasificador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumero_clases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: GMM() got an unexpected keyword argument 'n_components'"
     ]
    }
   ],
   "source": [
    "# MOSTRAMOS LOS DATOS DEL REPORTE.\n",
    "\n",
    "# Aplicación de métodos \n",
    "print('\\n\\n________________PROCESAMIENTO DE DATOS_____________')\n",
    "Preprocesamiento_datos(X,y)\n",
    "print('\\n\\n________________NAIVE BAYES________________________')\n",
    "Naive_Bayes(X,y)\n",
    "print('\\n\\n_________________Clustering________________________')\n",
    "k_means_Cluster(X,y)\n",
    "print('\\n\\n_________________Arboles de decisión_______________')\n",
    "Arbol_desicion(X,y)\n",
    "print('\\n\\n_________________Bosques aleatorios________________')\n",
    "bosques_aleatorios(X,y)\n",
    "print('\\n\\n_________________Mezclas Gausianas_________________')\n",
    "GMM(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Conclusion, para la base de datos en estudio, las tecnicas que mejor utilizan los datos, se dan en este orden: \n",
    "1. Procesamiento de datos\n",
    "2. Naive Bayes\n",
    "3. Bosques de decision\n",
    "4. Arboles de decision\n",
    "5. GMM\n",
    "6. Clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
